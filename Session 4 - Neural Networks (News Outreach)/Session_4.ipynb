{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIPTHNWxry-J"
      },
      "source": [
        "# **Data Cleaning and Neural Networks**\n",
        "\n",
        "##### Keshav Ramji and Emily Paul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BnzFxh9q7Tu"
      },
      "source": [
        "## **Set Up**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Y6gd94fQ87"
      },
      "source": [
        "First, we'll clone our AI@Penn GitHub repository, which contains all of our datasets. After running the following cell, you'll be able to see the contents of the repository in the file system on the left hand side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV9Y2mP0cjoK"
      },
      "source": [
        "!git clone https://github.com/kjaisingh/AI-Penn.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjMSeIeMhzoH"
      },
      "source": [
        "## **Data Cleaning: Black Friday Purchase Patterns**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6TyiN7KrpcO"
      },
      "source": [
        "Next, let's import the libraries we'll be using and name them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nETyKtuft4Bk"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# scikit-learn doesn't automatically import its subpackages, so we'll have to do this ourselves\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4HJR_6gDmyW"
      },
      "source": [
        "Now we can load the training and testing datasets (credit to Analytics Vidhya for the data):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDC2GAycT1IA"
      },
      "source": [
        "train = pd.read_csv('/content/AI-Penn/Session 4 - Neural Networks (News Outreach)/black_friday_train.csv')\n",
        "\n",
        "test = pd.read_csv('/content/AI-Penn/Session 4 - Neural Networks (News Outreach)/black_friday_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28KkYI6VLyvl"
      },
      "source": [
        "Print the training dataset to get a sense of what we're looking at:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37NpKYjeSZ-Q"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmMFKlhiS_2l"
      },
      "source": [
        "We can immediately see that we'll need to do some pre-processing. Several columns contain non-numerical data, which we'll have to encode, and there are missing values that we'll need to deal with. Also note that different columns of numerical data have different scales, so we should normalize the dataframes before moving on.\n",
        "\n",
        "First, let's delete the `User_ID` and `Product_ID` columns, since these labels don't encapsulate any qualities that might contribute to our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfVjqKpxd9FY"
      },
      "source": [
        "train = train.drop(labels=['User_ID', 'Product_ID'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfFp6NQsd4Ah"
      },
      "source": [
        "Now let's deal with the missing values; check to see which columns contain NaN values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR0kFOlaWdXs"
      },
      "source": [
        "train.columns[train.isnull().any()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJaTBDVMXHX0"
      },
      "source": [
        "So we know now that only the `Product_Category_2` and `Product_Category_3` have missing values. In this case, since the values in both of those columns represent the number of items purchased, it may be that the missing values are actually just a lack of purchases of goods in their categories (so they should actually be 0's). Note that in reality, you should **never make assumptions** about what missing values mean, so you would do more research about the dataset and/or run your analysis multiple times to gauge the effects of different types of NaN replacement. For now though, let's just replace the NaN's in these two columns with 0's:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EmxAa2MammZ"
      },
      "source": [
        "train['Product_Category_2'] = train['Product_Category_2'].fillna(0)\n",
        "train['Product_Category_3'] = train['Product_Category_3'].fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7Wuc7tCblFS"
      },
      "source": [
        "Now checking for NaN's should produce no columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcpmY7xSbtgu"
      },
      "source": [
        "train.columns[train.isnull().any()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYNQtfE9cHsq"
      },
      "source": [
        "Ok, next let's handle our categorical data. Check to see which columns contain non-numerical data by generating a list of columns whose data types are not numbers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRTHAvAdcSBJ"
      },
      "source": [
        "train.select_dtypes(exclude=['number'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_P9iAJ6dRen"
      },
      "source": [
        "For now, let's simply implement label encoding, but keep in mind that since we always want to **avoid introducing bias** we might use a different approach (e.g. one-hot encoding) in reality if that suits the data better. \n",
        "\n",
        "We'll use the label encoder provided by the scikit-learn library. This assigns a numerical value to each instance of a categorical feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zNROWnKh6Z5"
      },
      "source": [
        "label_encoder = preprocessing.LabelEncoder()\n",
        "train['Gender'] = label_encoder.fit_transform(train['Gender'])\n",
        "train['Age'] = label_encoder.fit_transform(train['Age'])\n",
        "train['City_Category'] = label_encoder.fit_transform(train['City_Category'])\n",
        "train['Stay_In_Current_City_Years'] = label_encoder.fit_transform(train['Stay_In_Current_City_Years'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLmTBpcEjX6e"
      },
      "source": [
        "Now checking for non-numerical values should produce no columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S4aquPdjoWu"
      },
      "source": [
        "train.select_dtypes(exclude=['number'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1G_yl9nj1_x"
      },
      "source": [
        "The last thing we need to do before moving on is to normalize the dataframe. We can do this using the `MinMaxScaler` provided by scikit-learn to scale the occurences of each feature to lie in the range from 0 to 1 when given no arguments:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym1AJZNVj_gE"
      },
      "source": [
        "scaler = preprocessing.MinMaxScaler()\n",
        "scaled_train = scaler.fit_transform(train)\n",
        "\n",
        "# the MinMaxScaler outputs a numpy ndarray, which we'll convert back into a pandas dataframe\n",
        "train = pd.DataFrame(scaled_train, columns = train.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VEG9CxQmJ8-"
      },
      "source": [
        "Let's take a look at our processed training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPXj2-EVmVws"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RlLADFTohp4"
      },
      "source": [
        "This is exactly what we wanted! Now let's pre-process our testing dataset the same way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPjuFyCyouvb"
      },
      "source": [
        "# drop the useless columns\n",
        "test = test.drop(labels=['User_ID', 'Product_ID'], axis = 1)\n",
        "\n",
        "# replace NaN's with 0's\n",
        "test['Product_Category_2'] = test['Product_Category_2'].fillna(0)\n",
        "test['Product_Category_3'] = test['Product_Category_3'].fillna(0)\n",
        "\n",
        "# implement label encoding on the categorical values\n",
        "# note that there's no need to re-instantiate the label encoder\n",
        "test['Gender'] = label_encoder.fit_transform(test['Gender'])\n",
        "test['Age'] = label_encoder.fit_transform(test['Age'])\n",
        "test['City_Category'] = label_encoder.fit_transform(test['City_Category'])\n",
        "test['Stay_In_Current_City_Years'] = label_encoder.fit_transform(test['Stay_In_Current_City_Years'])\n",
        "\n",
        "# scale the dataframe\n",
        "# note that there's no need to re-instantiate the scaler\n",
        "scaled_test = scaler.fit_transform(test)\n",
        "test = pd.DataFrame(scaled_test, columns = test.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S-cYGttsoXs"
      },
      "source": [
        "So checking for NaN's shouldn't produce any columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yv0TrwdsukQ"
      },
      "source": [
        "test.columns[test.isnull().any()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgglUUBGs3CE"
      },
      "source": [
        "And neither should checking for non-numerical values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xal7IdIGs9kD"
      },
      "source": [
        "test.select_dtypes(exclude=['number'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPnF3pIAuMsj"
      },
      "source": [
        "Printing the cleaned testing dataframe shows that it now looks to be in the same format as our cleaned training dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj_4Yihvuouz"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m8PPL8gqB-x"
      },
      "source": [
        "We're all set; at this point you could start to run an analysis on these cleaned datasets. \n",
        "\n",
        "Let's move on to exploring how to build a neural network!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaI3pnWurIaS"
      },
      "source": [
        "## **Neural Networks: Predicting Audiobook Sales from Prior Purchases**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tc8omkrqghG"
      },
      "source": [
        "We're going to approach this using Tensorflow, which is another framework/library which is widely used for machine learning. \n",
        "\n",
        "Tensorflow 2.0, the version we'll be using here, has been integrated with Keras (another commonly used framework) as its front-end. \n",
        "Let's import the packages we'll be using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVAJ_4X_UUOx"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VFOFalPUn6Y"
      },
      "source": [
        "Our goal is to build a neural network that can predict audio book sales from information about customers' prior purchases. Let's take a look at our dataset (credit to 365DataScience for the data):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83wjLmajGr2H"
      },
      "source": [
        "dataset = pd.read_csv('/content/AI-Penn/Session 4 - Neural Networks (News Outreach)/Audiobooks_data_3_with_titles.csv')\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfBNJR2ZHqTf"
      },
      "source": [
        "Our objective here will be to use the other features to predict the targets column. The targets column displays 1 if a user who has previously purchased an audiobook becomes a repeat customer, and 0 if they do not return. \n",
        "\n",
        "We'll start by loading in our datasets - these are `.npz` files, which are a form of storing numpy arrays. Don't worry too much about the preprocessing - we've already done it for you, and the resulting train, test, and validation `.npz` files are in our repository. \n",
        "\n",
        "If you're interested in how the pre-processing was done with numpy, you can take a look at the code block in the appendix at the bottom of this Colaboratory notebook. Running that cell will load `Audiobooks_data_3.csv` and generate the train, test, and validation `.npz` files for you.\n",
        "\n",
        "Let's load our cleaned training, validation, and testing datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAVfm4mPt5Ch"
      },
      "source": [
        "train = np.load('/content/AI-Penn/Session 4 - Neural Networks (News Outreach)/Audiobooks_train_data_3.npz')\n",
        "train_inputs = train['inputs'].astype(np.float)\n",
        "train_targets = train['targets'].astype(np.int)\n",
        "\n",
        "validation = np.load('/content/AI-Penn/Session 4 - Neural Networks (News Outreach)/Audiobooks_validation_data_3.npz')\n",
        "validation_inputs = validation['inputs'].astype(np.float)\n",
        "validation_targets =  validation['targets'].astype(np.int)\n",
        "\n",
        "test = np.load('/content/AI-Penn/Session 4 - Neural Networks (News Outreach)/Audiobooks_test_data_3.npz')\n",
        "test_inputs = test['inputs'].astype(np.float)\n",
        "test_targets = test['targets'].astype(np.int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z82DwZdq1mi"
      },
      "source": [
        "Now let's declare how many inputs, outputs, and nodes per hidden layer we want our neural network to have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDHVo5wtEMk2"
      },
      "source": [
        "input_size = 10 # This is because there are 10 columns that we use (not including ID) in our prediction\n",
        "output_size = 2 # This is because there are 2 possible outputs - 1 (yes, customer returned) or 0 (no, did not return)\n",
        "hidden_layer_nodes = 100 # can be chosen as seen fit - this is part of hyperparameter tuning, to see what size would be optimal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ltF6ujVrHhG"
      },
      "source": [
        "We'll now build our neural network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnZfVwyjHghO"
      },
      "source": [
        "tf.keras.initializers.GlorotNormal(seed=None)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                 tf.keras.layers.Input(shape = input_size), \n",
        "                 tf.keras.layers.Dense(hidden_layer_nodes, activation='relu'), \n",
        "                 tf.keras.layers.Dense(hidden_layer_nodes, activation='relu'),\n",
        "                 #tf.keras.layers.Dropout(0.3,),\n",
        "                 tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 32 # This is the number of samples that we are considering for each iteration\n",
        "epochs = 100     # The number of passes through "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nbVGleVQpNx"
      },
      "source": [
        "Let's first make note of how the model is being constructed using `keras.Sequential`. We have 2 fully connected hidden layers with 100 nodes each with the ReLU activation function, and a final dense layer with the 2 output prediction nodes.\n",
        "\n",
        "When we run `model.compile()`, we are effectively configuring the neural network for optimization through the loss function (we have chosen \"sparse_categorical_crossentropy\") and the optimization method (Adam). Adam has a pre-set learning rate of 0.001, but we can change this as well. \n",
        "\n",
        "We chose sparse_categorical_crossentropy here because that is often used when we want to calculate the loss between the labels and predictions for a categorical variable, which we have here since we have 2 possible outputs - 0 or 1 - for the targets (did the customer return or not). \n",
        "\n",
        "It's important to note that there are many activation functions that may be used in various stages, but ReLU is very widely used for the hidden layers, and softmax or sigmoid is commonly used for the output layer. \n",
        "\n",
        "Here is some tensorflow documentation which goes through different loss and activation functions, as well as optimization methods, if anyone is interested in learning more about this. \n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/activations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP9jm2TQY_T4"
      },
      "source": [
        "Lastly, we need to fit the model - what we have done so far is like configuration, and what we now need to do is to actually train the neural network! Let's see our results: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CH2xNxyXHfA"
      },
      "source": [
        "model.fit(train_inputs, train_targets, batch_size = batch_size, \n",
        "          epochs = epochs, validation_data = (validation_inputs, validation_targets),\n",
        "          verbose = 2 # this is just to ensure that what is printed is loss, validation loss, accuracy, and validation accuracy\n",
        "          )  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmdbs-Ix6GrA"
      },
      "source": [
        "We can also work with the model on the test set, although we won't get as strong of a picture of what our results look like. We can do this through model.evaluate, which takes in the test set and the batch_size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbMljNnabMEb"
      },
      "source": [
        "model.evaluate(test_inputs, test_targets, batch_size = batch_size, verbose = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWCjl_eYZZxA"
      },
      "source": [
        "Our results are not too bad! We're getting around 83% on our validation accuracy, and we haven't even used any special techniques yet. The first thing that we can do is hyperparameter tuning - changing the size of the hidden layers, possibly the batch size, the number of epochs, and so on. \n",
        "\n",
        "We definitely also need to be mindful of overfitting - there are a few techniques that we can use to accomplish this which are mostly beyond the scope of this bootcamp - but be sure to keep an eye out for future sessions where we may cover topics like these! However, a very easy that method we can employ is dropout, where we effectively ignore a certain percentage of inputs for each iteration, and this allows us to learn various parts of the data better. \n",
        "\n",
        "For now, instead of constantly running this for 100 epochs even when validation loss continues to increase, we will use Tensorflow's `EarlyStopping` mechanism to stop fitting the model after a \"certain number of iterations\" have passed where the loss was higher than the previous epoch's loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5ivxE3Tbsvl"
      },
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(patience = 5) # This patience is the \"certain number of iterations\" mentioned above\n",
        "model.fit(train_inputs, train_targets, batch_size = batch_size, \n",
        "          epochs = epochs, validation_data=(validation_inputs, validation_targets), \n",
        "          verbose = 2, \n",
        "          callbacks = [stop_early]\n",
        "          )  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grxOQSEPcI0Z"
      },
      "source": [
        "Note, however, that the main drawback to using the `EarlyStopping` mechanism is that we do not get to see the broader set of validation accuracy results - this leaves us to believe that our maximum accuracy is lower than it actually is, and does not allow us to get a sense of general consistency of performance - every percent in accuracy counts!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVYUvvY2_oyF"
      },
      "source": [
        "#**Appendix**\n",
        "Here are the preprocessing steps for the audiobooks customer dataset that we didn't touch upon for the neural networks component of the programming session. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTLD5byaTjfw"
      },
      "source": [
        "# preprocessing steps\n",
        "csv_data = np.loadtxt('/content/AI-Penn/Session 4 - Neural Networks (News Outreach)/Audiobooks_data_3.csv', delimiter = ',')\n",
        "unscaled_inputs = csv_data [:,1:-1]\n",
        "targets = csv_data[:,-1]\n",
        "\n",
        "# balancing dataset - ensuring that when we train, we are able to ensure that we have similar number of each so\n",
        "# that our model does not learn one more than the other\n",
        "num_one_targets = int(np.sum(targets))  ## find number of 1 boolean values for balancing\n",
        "num_zero_targets = 0\n",
        "removed_index = []\n",
        "for i in range (targets.shape[0]):\n",
        "    if(targets[i] == 0):\n",
        "        num_zero_targets += 1\n",
        "        if (num_zero_targets > num_one_targets):\n",
        "            removed_index.append(i)\n",
        "unscaled_inputs_equalpriors = np.delete (unscaled_inputs, removed_index, axis = 0)\n",
        "target_equal_priors = np.delete (targets, removed_index, axis = 0)\n",
        "\n",
        "# Scaling the set using sklearn's preprocessing \n",
        "scaled_inputs = preprocessing.scale(unscaled_inputs_equalpriors)\n",
        "\n",
        "#shuffling - so we effectively have a randomized process of selecting values from the dataset for when it is partitioned\n",
        "indices_shuffled = np.arange(scaled_inputs.shape[0])\n",
        "np.random.shuffle(indices_shuffled)\n",
        "shuffled_inputs = scaled_inputs[indices_shuffled]\n",
        "shuffled_targets = target_equal_priors[indices_shuffled]\n",
        "\n",
        "# splitting data into train, validation, and test -- currently 80:10:10 but can be tweaked\n",
        "samples_count = shuffled_inputs.shape[0]\n",
        "samples_count = scaled_inputs.shape[0]\n",
        "train_samples_count = int(0.8*samples_count) \n",
        "validation_samples_count = int(0.1*samples_count)\n",
        "test_samples_count = samples_count - train_samples_count - validation_samples_count \n",
        "\n",
        "#Setting the train, validation, and test inputs and outputs to incorporate particular ranges of the set \n",
        "train_inputs = shuffled_inputs[:train_samples_count]\n",
        "train_targets = shuffled_targets[:train_samples_count]\n",
        "#train_inputs = scaled_inputs[:train_samples_count]\n",
        "#train_targets = target_equal_priors[:train_samples_count]\n",
        "\n",
        "validation_inputs = shuffled_inputs[train_samples_count:(train_samples_count + validation_samples_count)]\n",
        "validation_targets = shuffled_targets[train_samples_count:(train_samples_count + validation_samples_count)]\n",
        "\n",
        "test_inputs = shuffled_inputs[(train_samples_count + validation_samples_count):]\n",
        "test_targets = shuffled_targets[(train_samples_count + validation_samples_count):]\n",
        "\n",
        "\n",
        "print(np.sum(train_targets), train_samples_count, np.sum(train_targets)/train_samples_count)\n",
        "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets)/validation_samples_count)\n",
        "print(np.sum(test_targets), test_samples_count, np.sum(test_targets)/test_samples_count)\n",
        "\n",
        "# Saving the numpy arrays as .npz files - this is what is in the Github that we imported to construct our neural network from\n",
        "np.savez('Audiobooks_train_data_2', inputs=train_inputs, targets=train_targets)\n",
        "np.savez('Audiobooks_validation_data_2',inputs=validation_inputs, targets=validation_targets)\n",
        "np.savez('Audiobooks_test_data_2', inputs=test_inputs, targets=test_targets)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}