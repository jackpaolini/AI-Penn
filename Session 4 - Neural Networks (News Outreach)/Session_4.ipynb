{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIPTHNWxry-J"
      },
      "source": [
        "# **Data Cleaning and Neural Networks**\n",
        "\n",
        "##### Keshav Ramji and Emily Paul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BnzFxh9q7Tu"
      },
      "source": [
        "## **Set Up**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Y6gd94fQ87"
      },
      "source": [
        "First, we'll clone our AI@Penn GitHub repository, which contains all of our datasets. After running the following cell, you'll be able to see the contents of the repository in the file system on the left hand side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV9Y2mP0cjoK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4ce69a-854a-477a-ad21-fb7541b56b64"
      },
      "source": [
        "!git clone https://github.com/kjaisingh/AI-Penn.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AI-Penn'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 222 (delta 26), reused 0 (delta 0), pack-reused 178\u001b[K\n",
            "Receiving objects: 100% (222/222), 50.87 MiB | 27.02 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjMSeIeMhzoH"
      },
      "source": [
        "## **Data Cleaning: Black Friday Purchase Patterns**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6TyiN7KrpcO"
      },
      "source": [
        "Next, let's import the libraries we'll be using and name them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nETyKtuft4Bk"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# scikit-learn doesn't automatically import its subpackages, so we'll have to do this ourselves\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4HJR_6gDmyW"
      },
      "source": [
        "Now we can load the training and testing datasets (credit to Analytics Vidhya for the data):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDC2GAycT1IA"
      },
      "source": [
        "train = pd.read_csv('/content/AI-Penn/Session 4 - Neural Networks (News Outreach)/black_friday_train.csv')\n",
        "\n",
        "test = pd.read_csv('/content/AI-Penn/Session 4 - Neural Networks (News Outreach)/black_friday_test.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28KkYI6VLyvl"
      },
      "source": [
        "Print the training dataset to get a sense of what we're looking at:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37NpKYjeSZ-Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "85ffa766-6501-4219-a7f5-6bbd6be68379"
      },
      "source": [
        "train"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User_ID</th>\n",
              "      <th>Product_ID</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>City_Category</th>\n",
              "      <th>Stay_In_Current_City_Years</th>\n",
              "      <th>Marital_Status</th>\n",
              "      <th>Product_Category_1</th>\n",
              "      <th>Product_Category_2</th>\n",
              "      <th>Product_Category_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00069042</td>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00248942</td>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.0</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00087842</td>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00085442</td>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000002</td>\n",
              "      <td>P00285442</td>\n",
              "      <td>M</td>\n",
              "      <td>55+</td>\n",
              "      <td>16</td>\n",
              "      <td>C</td>\n",
              "      <td>4+</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550063</th>\n",
              "      <td>1006033</td>\n",
              "      <td>P00372445</td>\n",
              "      <td>M</td>\n",
              "      <td>51-55</td>\n",
              "      <td>13</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550064</th>\n",
              "      <td>1006035</td>\n",
              "      <td>P00375436</td>\n",
              "      <td>F</td>\n",
              "      <td>26-35</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550065</th>\n",
              "      <td>1006036</td>\n",
              "      <td>P00375436</td>\n",
              "      <td>F</td>\n",
              "      <td>26-35</td>\n",
              "      <td>15</td>\n",
              "      <td>B</td>\n",
              "      <td>4+</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550066</th>\n",
              "      <td>1006038</td>\n",
              "      <td>P00375436</td>\n",
              "      <td>F</td>\n",
              "      <td>55+</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550067</th>\n",
              "      <td>1006039</td>\n",
              "      <td>P00371644</td>\n",
              "      <td>F</td>\n",
              "      <td>46-50</td>\n",
              "      <td>0</td>\n",
              "      <td>B</td>\n",
              "      <td>4+</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>550068 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        User_ID Product_ID  ... Product_Category_2 Product_Category_3\n",
              "0       1000001  P00069042  ...                NaN                NaN\n",
              "1       1000001  P00248942  ...                6.0               14.0\n",
              "2       1000001  P00087842  ...                NaN                NaN\n",
              "3       1000001  P00085442  ...               14.0                NaN\n",
              "4       1000002  P00285442  ...                NaN                NaN\n",
              "...         ...        ...  ...                ...                ...\n",
              "550063  1006033  P00372445  ...                NaN                NaN\n",
              "550064  1006035  P00375436  ...                NaN                NaN\n",
              "550065  1006036  P00375436  ...                NaN                NaN\n",
              "550066  1006038  P00375436  ...                NaN                NaN\n",
              "550067  1006039  P00371644  ...                NaN                NaN\n",
              "\n",
              "[550068 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmMFKlhiS_2l"
      },
      "source": [
        "We can immediately see that we'll need to do some pre-processing. Several columns contain non-numerical data, which we'll have to encode, and there are missing values that we'll need to deal with. Also note that different columns of numerical data have different scales, so we should normalize the dataframes before moving on.\n",
        "\n",
        "First, let's delete the `User_ID` and `Product_ID` columns, since these labels don't encapsulate any qualities that might contribute to our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfVjqKpxd9FY"
      },
      "source": [
        "train = train.drop(labels=['User_ID', 'Product_ID'], axis = 1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfFp6NQsd4Ah"
      },
      "source": [
        "Now let's deal with the missing values; check to see which columns contain NaN values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR0kFOlaWdXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b01af8-6728-492d-f597-82775f614ecc"
      },
      "source": [
        "train.columns[train.isnull().any()]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Product_Category_2', 'Product_Category_3'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJaTBDVMXHX0"
      },
      "source": [
        "So we know now that only the `Product_Category_2` and `Product_Category_3` have missing values. In this case, since the values in both of those columns represent the number of items purchased, it may be that the missing values are actually just a lack of purchases of goods in their categories (so they should actually be 0's). Note that in reality, you should **never make assumptions** about what missing values mean, so you would do more research about the dataset and/or run your analysis multiple times to gauge the effects of different types of NaN replacement. For now though, let's just replace the NaN's in these two columns with 0's:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EmxAa2MammZ"
      },
      "source": [
        "train['Product_Category_2'] = train['Product_Category_2'].fillna(0)\n",
        "train['Product_Category_3'] = train['Product_Category_3'].fillna(0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7Wuc7tCblFS"
      },
      "source": [
        "Now checking for NaN's should produce no columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcpmY7xSbtgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8174142-85c8-420f-c899-94bbcba3be3c"
      },
      "source": [
        "train.columns[train.isnull().any()]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYNQtfE9cHsq"
      },
      "source": [
        "Ok, next let's handle our categorical data. Check to see which columns contain non-numerical data by generating a list of columns whose data types are not numbers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRTHAvAdcSBJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "9a16de35-ef12-4757-f387-57b3948ece8d"
      },
      "source": [
        "train.select_dtypes(exclude=['number'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>City_Category</th>\n",
              "      <th>Stay_In_Current_City_Years</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>M</td>\n",
              "      <td>55+</td>\n",
              "      <td>C</td>\n",
              "      <td>4+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550063</th>\n",
              "      <td>M</td>\n",
              "      <td>51-55</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550064</th>\n",
              "      <td>F</td>\n",
              "      <td>26-35</td>\n",
              "      <td>C</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550065</th>\n",
              "      <td>F</td>\n",
              "      <td>26-35</td>\n",
              "      <td>B</td>\n",
              "      <td>4+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550066</th>\n",
              "      <td>F</td>\n",
              "      <td>55+</td>\n",
              "      <td>C</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550067</th>\n",
              "      <td>F</td>\n",
              "      <td>46-50</td>\n",
              "      <td>B</td>\n",
              "      <td>4+</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>550068 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Gender    Age City_Category Stay_In_Current_City_Years\n",
              "0           F   0-17             A                          2\n",
              "1           F   0-17             A                          2\n",
              "2           F   0-17             A                          2\n",
              "3           F   0-17             A                          2\n",
              "4           M    55+             C                         4+\n",
              "...       ...    ...           ...                        ...\n",
              "550063      M  51-55             B                          1\n",
              "550064      F  26-35             C                          3\n",
              "550065      F  26-35             B                         4+\n",
              "550066      F    55+             C                          2\n",
              "550067      F  46-50             B                         4+\n",
              "\n",
              "[550068 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_P9iAJ6dRen"
      },
      "source": [
        "For now, let's simply implement label encoding, but keep in mind that since we always want to **avoid introducing bias** we might use a different approach (e.g. one-hot encoding) in reality if that suits the data better. \n",
        "\n",
        "We'll use the label encoder provided by the scikit-learn library. This assigns a numerical value to each instance of a categorical feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zNROWnKh6Z5"
      },
      "source": [
        "label_encoder = preprocessing.LabelEncoder()\n",
        "train['Gender'] = label_encoder.fit_transform(train['Gender'])\n",
        "train['Age'] = label_encoder.fit_transform(train['Age'])\n",
        "train['City_Category'] = label_encoder.fit_transform(train['City_Category'])\n",
        "train['Stay_In_Current_City_Years'] = label_encoder.fit_transform(train['Stay_In_Current_City_Years'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLmTBpcEjX6e"
      },
      "source": [
        "Now checking for non-numerical values should produce no columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S4aquPdjoWu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "02e27f60-0c97-4635-9427-49b192251974"
      },
      "source": [
        "train.select_dtypes(exclude=['number'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550063</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550064</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550065</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550066</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550067</th>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>550068 rows × 0 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
              "\n",
              "[550068 rows x 0 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1G_yl9nj1_x"
      },
      "source": [
        "The last thing we need to do before moving on is to normalize the dataframe. We can do this using the `MinMaxScaler` provided by scikit-learn to scale the occurences of each feature to lie in the range from 0 to 1 when given no arguments:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym1AJZNVj_gE"
      },
      "source": [
        "scaler = preprocessing.MinMaxScaler()\n",
        "scaled_train = scaler.fit_transform(train)\n",
        "\n",
        "# the MinMaxScaler outputs a numpy ndarray, which we'll convert back into a pandas dataframe\n",
        "train = pd.DataFrame(scaled_train, columns = train.columns)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VEG9CxQmJ8-"
      },
      "source": [
        "Let's take a look at our processed training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPXj2-EVmVws",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "1bcf316d-ba59-4d74-8339-e2a28c58e1b0"
      },
      "source": [
        "train"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>City_Category</th>\n",
              "      <th>Stay_In_Current_City_Years</th>\n",
              "      <th>Marital_Status</th>\n",
              "      <th>Product_Category_1</th>\n",
              "      <th>Product_Category_2</th>\n",
              "      <th>Product_Category_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.105263</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.777778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.80</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.368421</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550063</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550064</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550065</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550066</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550067</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>550068 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Gender       Age  ...  Product_Category_2  Product_Category_3\n",
              "0          0.0  0.000000  ...            0.000000            0.000000\n",
              "1          0.0  0.000000  ...            0.333333            0.777778\n",
              "2          0.0  0.000000  ...            0.000000            0.000000\n",
              "3          0.0  0.000000  ...            0.777778            0.000000\n",
              "4          1.0  1.000000  ...            0.000000            0.000000\n",
              "...        ...       ...  ...                 ...                 ...\n",
              "550063     1.0  0.833333  ...            0.000000            0.000000\n",
              "550064     0.0  0.333333  ...            0.000000            0.000000\n",
              "550065     0.0  0.333333  ...            0.000000            0.000000\n",
              "550066     0.0  1.000000  ...            0.000000            0.000000\n",
              "550067     0.0  0.666667  ...            0.000000            0.000000\n",
              "\n",
              "[550068 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RlLADFTohp4"
      },
      "source": [
        "This is exactly what we wanted! Now let's pre-process our testing dataset the same way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPjuFyCyouvb"
      },
      "source": [
        "# drop the useless columns\n",
        "test = test.drop(labels=['User_ID', 'Product_ID'], axis = 1)\n",
        "\n",
        "# replace NaN's with 0's\n",
        "test['Product_Category_2'] = test['Product_Category_2'].fillna(0)\n",
        "test['Product_Category_3'] = test['Product_Category_3'].fillna(0)\n",
        "\n",
        "# implement label encoding on the categorical values\n",
        "# note that there's no need to re-instantiate the label encoder\n",
        "test['Gender'] = label_encoder.fit_transform(test['Gender'])\n",
        "test['Age'] = label_encoder.fit_transform(test['Age'])\n",
        "test['City_Category'] = label_encoder.fit_transform(test['City_Category'])\n",
        "test['Stay_In_Current_City_Years'] = label_encoder.fit_transform(test['Stay_In_Current_City_Years'])\n",
        "\n",
        "# scale the dataframe\n",
        "# note that there's no need to re-instantiate the scaler\n",
        "scaled_test = scaler.fit_transform(test)\n",
        "test = pd.DataFrame(scaled_test, columns = test.columns)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S-cYGttsoXs"
      },
      "source": [
        "So checking for NaN's shouldn't produce any columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yv0TrwdsukQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32d730bd-c9dd-49da-f495-ea90ad044390"
      },
      "source": [
        "test.columns[test.isnull().any()]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgglUUBGs3CE"
      },
      "source": [
        "And neither should checking for non-numerical values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xal7IdIGs9kD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "8d0a4b43-2be1-4ebe-e325-6a0a337e31c6"
      },
      "source": [
        "test.select_dtypes(exclude=['number'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233594</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233595</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233596</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233597</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233598</th>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>233599 rows × 0 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
              "\n",
              "[233599 rows x 0 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPnF3pIAuMsj"
      },
      "source": [
        "Printing the cleaned testing dataframe shows that it now looks to be in the same format as our cleaned training dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj_4Yihvuouz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "2ebfc86b-0e90-4aa8-a9dc-a64f4b116afe"
      },
      "source": [
        "test"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>City_Category</th>\n",
              "      <th>Stay_In_Current_City_Years</th>\n",
              "      <th>Marital_Status</th>\n",
              "      <th>Product_Category_1</th>\n",
              "      <th>Product_Category_2</th>\n",
              "      <th>Product_Category_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.85</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233594</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.411765</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233595</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233596</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233597</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233598</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>233599 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Gender       Age  ...  Product_Category_2  Product_Category_3\n",
              "0          1.0  0.666667  ...            0.611111            0.000000\n",
              "1          1.0  0.333333  ...            0.277778            0.000000\n",
              "2          0.0  0.500000  ...            0.777778            0.000000\n",
              "3          0.0  0.500000  ...            0.500000            0.000000\n",
              "4          0.0  0.333333  ...            0.277778            0.666667\n",
              "...        ...       ...  ...                 ...                 ...\n",
              "233594     0.0  0.333333  ...            0.000000            0.000000\n",
              "233595     0.0  0.333333  ...            0.444444            0.000000\n",
              "233596     0.0  0.333333  ...            0.277778            0.666667\n",
              "233597     0.0  0.666667  ...            0.888889            0.000000\n",
              "233598     0.0  0.666667  ...            0.277778            0.000000\n",
              "\n",
              "[233599 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m8PPL8gqB-x"
      },
      "source": [
        "We're all set; at this point you could start to run an analysis on these cleaned datasets. \n",
        "\n",
        "Let's move on to exploring how to build a neural network!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaI3pnWurIaS"
      },
      "source": [
        "## **Neural Networks: Predicting Audiobook Sales from Prior Purchases**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tc8omkrqghG"
      },
      "source": [
        "We're going to approach this using Tensorflow, which is another framework/library which is widely used for machine learning. \n",
        "\n",
        "Tensorflow 2.0, the version we'll be using here, has been integrated with Keras (another commonly used framework) as its front-end. \n",
        "Let's import the packages we'll be using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVAJ_4X_UUOx"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VFOFalPUn6Y"
      },
      "source": [
        "Our goal is to build a neural network that can predict audio book sales from information about customers' prior purchases. Let's take a look at our dataset (credit to 365DataScience for the data):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83wjLmajGr2H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "e5eb8ed0-7cb8-4f9f-a5a8-36fec9d07a44"
      },
      "source": [
        "dataset = pd.read_csv('/content/AI-Penn/Session 4 - Neural Networks (News Outreach)/Audiobooks_data_3_with_titles.csv')\n",
        "dataset"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Book_length_overall</th>\n",
              "      <th>Book_length_avg</th>\n",
              "      <th>Price_overall</th>\n",
              "      <th>Price_avg</th>\n",
              "      <th>Gave_review</th>\n",
              "      <th>Review_on_10</th>\n",
              "      <th>Completion</th>\n",
              "      <th>Minutes_Listened</th>\n",
              "      <th>Support_required</th>\n",
              "      <th>days_between_purchase_and_visit</th>\n",
              "      <th>Targets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>994</td>\n",
              "      <td>1620.0</td>\n",
              "      <td>1620</td>\n",
              "      <td>19.73</td>\n",
              "      <td>19.73</td>\n",
              "      <td>1</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0.99</td>\n",
              "      <td>1603.8</td>\n",
              "      <td>5</td>\n",
              "      <td>92</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1143</td>\n",
              "      <td>2160.0</td>\n",
              "      <td>2160</td>\n",
              "      <td>5.33</td>\n",
              "      <td>5.33</td>\n",
              "      <td>0</td>\n",
              "      <td>8.91</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2059</td>\n",
              "      <td>2160.0</td>\n",
              "      <td>2160</td>\n",
              "      <td>5.33</td>\n",
              "      <td>5.33</td>\n",
              "      <td>0</td>\n",
              "      <td>8.91</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>388</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2882</td>\n",
              "      <td>1620.0</td>\n",
              "      <td>1620</td>\n",
              "      <td>5.96</td>\n",
              "      <td>5.96</td>\n",
              "      <td>0</td>\n",
              "      <td>8.91</td>\n",
              "      <td>0.42</td>\n",
              "      <td>680.4</td>\n",
              "      <td>1</td>\n",
              "      <td>129</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3342</td>\n",
              "      <td>2160.0</td>\n",
              "      <td>2160</td>\n",
              "      <td>5.33</td>\n",
              "      <td>5.33</td>\n",
              "      <td>0</td>\n",
              "      <td>8.91</td>\n",
              "      <td>0.22</td>\n",
              "      <td>475.2</td>\n",
              "      <td>0</td>\n",
              "      <td>361</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14079</th>\n",
              "      <td>28220</td>\n",
              "      <td>1620.0</td>\n",
              "      <td>1620</td>\n",
              "      <td>5.33</td>\n",
              "      <td>5.33</td>\n",
              "      <td>1</td>\n",
              "      <td>9.00</td>\n",
              "      <td>0.61</td>\n",
              "      <td>988.2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14080</th>\n",
              "      <td>28671</td>\n",
              "      <td>1080.0</td>\n",
              "      <td>1080</td>\n",
              "      <td>6.55</td>\n",
              "      <td>6.55</td>\n",
              "      <td>1</td>\n",
              "      <td>6.00</td>\n",
              "      <td>0.29</td>\n",
              "      <td>313.2</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14081</th>\n",
              "      <td>31134</td>\n",
              "      <td>2160.0</td>\n",
              "      <td>2160</td>\n",
              "      <td>6.14</td>\n",
              "      <td>6.14</td>\n",
              "      <td>0</td>\n",
              "      <td>8.91</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14082</th>\n",
              "      <td>32832</td>\n",
              "      <td>1620.0</td>\n",
              "      <td>1620</td>\n",
              "      <td>5.33</td>\n",
              "      <td>5.33</td>\n",
              "      <td>1</td>\n",
              "      <td>8.00</td>\n",
              "      <td>0.38</td>\n",
              "      <td>615.6</td>\n",
              "      <td>0</td>\n",
              "      <td>90</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14083</th>\n",
              "      <td>251</td>\n",
              "      <td>1674.0</td>\n",
              "      <td>3348</td>\n",
              "      <td>5.33</td>\n",
              "      <td>10.67</td>\n",
              "      <td>0</td>\n",
              "      <td>8.91</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14084 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          ID  Book_length_overall  ...  days_between_purchase_and_visit  Targets\n",
              "0        994               1620.0  ...                               92        0\n",
              "1       1143               2160.0  ...                                0        0\n",
              "2       2059               2160.0  ...                              388        0\n",
              "3       2882               1620.0  ...                              129        0\n",
              "4       3342               2160.0  ...                              361        0\n",
              "...      ...                  ...  ...                              ...      ...\n",
              "14079  28220               1620.0  ...                                4        0\n",
              "14080  28671               1080.0  ...                               29        0\n",
              "14081  31134               2160.0  ...                                0        0\n",
              "14082  32832               1620.0  ...                               90        0\n",
              "14083    251               1674.0  ...                                0        1\n",
              "\n",
              "[14084 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfBNJR2ZHqTf"
      },
      "source": [
        "Our objective here will be to use the other features to predict the targets column. The targets column displays 1 if a user who has previously purchased an audiobook becomes a repeat customer, and 0 if they do not return. \n",
        "\n",
        "We'll start by loading in our datasets - these are `.npz` files, which are a form of storing numpy arrays. Don't worry too much about the preprocessing - we've already done it for you, and the resulting train, test, and validation `.npz` files are in our repository. \n",
        "\n",
        "If you're interested in how the pre-processing was done with numpy, you can take a look at the code block in the appendix at the bottom of this Colaboratory notebook. Running that cell will load `Audiobooks_data_3.csv` and generate the train, test, and validation `.npz` files for you.\n",
        "\n",
        "Let's load our cleaned training, validation, and testing datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAVfm4mPt5Ch"
      },
      "source": [
        "train = np.load('/content/AI-Penn/Session 4 - Neural Networks (News Outreach)/Audiobooks_train_data_3.npz')\n",
        "train_inputs = train['inputs'].astype(np.float)\n",
        "train_targets = train['targets'].astype(np.int)\n",
        "\n",
        "validation = np.load('/content/AI-Penn/Session 4 - Neural Networks (News Outreach)/Audiobooks_validation_data_3.npz')\n",
        "validation_inputs = validation['inputs'].astype(np.float)\n",
        "validation_targets =  validation['targets'].astype(np.int)\n",
        "\n",
        "test = np.load('/content/AI-Penn/Session 4 - Neural Networks (News Outreach)/Audiobooks_test_data_3.npz')\n",
        "test_inputs = test['inputs'].astype(np.float)\n",
        "test_targets = test['targets'].astype(np.int)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z82DwZdq1mi"
      },
      "source": [
        "Now let's declare how many inputs, outputs, and nodes per hidden layer we want our neural network to have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDHVo5wtEMk2"
      },
      "source": [
        "input_size = 10 # This is because there are 10 columns that we use (not including ID) in our prediction\n",
        "output_size = 2 # This is because there are 2 possible outputs - 1 (yes, customer returned) or 0 (no, did not return)\n",
        "hidden_layer_nodes = 100 # can be chosen as seen fit - this is part of hyperparameter tuning, to see what size would be optimal"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ltF6ujVrHhG"
      },
      "source": [
        "We'll now build our neural network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnZfVwyjHghO"
      },
      "source": [
        "tf.keras.initializers.GlorotNormal(seed=None)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                 tf.keras.layers.Input(shape = input_size), \n",
        "                 tf.keras.layers.Dense(hidden_layer_nodes, activation='relu'), \n",
        "                 tf.keras.layers.Dense(hidden_layer_nodes, activation='relu'),\n",
        "                 #tf.keras.layers.Dropout(0.3,),\n",
        "                 tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 32 # This is the number of samples that we are considering for each iteration\n",
        "epochs = 100     # The number of passes through "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nbVGleVQpNx"
      },
      "source": [
        "Let's first make note of how the model is being constructed using `keras.Sequential`. We have 2 fully connected hidden layers with 100 nodes each with the ReLU activation function, and a final dense layer with the 2 output prediction nodes.\n",
        "\n",
        "When we run `model.compile()`, we are effectively configuring the neural network for optimization through the loss function (we have chosen \"sparse_categorical_crossentropy\") and the optimization method (Adam). Adam has a pre-set learning rate of 0.001, but we can change this as well. \n",
        "\n",
        "We chose sparse_categorical_crossentropy here because that is often used when we want to calculate the loss between the labels and predictions for a categorical variable, which we have here since we have 2 possible outputs - 0 or 1 - for the targets (did the customer return or not). \n",
        "\n",
        "It's important to note that there are many activation functions that may be used in various stages, but ReLU is very widely used for the hidden layers, and softmax or sigmoid is commonly used for the output layer. \n",
        "\n",
        "Here is some tensorflow documentation which goes through different loss and activation functions, as well as optimization methods, if anyone is interested in learning more about this. \n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/activations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP9jm2TQY_T4"
      },
      "source": [
        "Lastly, we need to fit the model - what we have done so far is like configuration, and what we now need to do is to actually train the neural network! Let's see our results: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CH2xNxyXHfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c7fd84a-58bc-425d-a3e1-e4b546abfb18"
      },
      "source": [
        "model.fit(train_inputs, train_targets, batch_size = batch_size, \n",
        "          epochs = epochs, validation_data = (validation_inputs, validation_targets),\n",
        "          verbose = 2 # this is just to ensure that what is printed is loss, validation loss, accuracy, and validation accuracy\n",
        "          )  "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "112/112 - 0s - loss: 0.4769 - accuracy: 0.7390 - val_loss: 0.4221 - val_accuracy: 0.7539\n",
            "Epoch 2/100\n",
            "112/112 - 0s - loss: 0.3746 - accuracy: 0.7938 - val_loss: 0.3865 - val_accuracy: 0.8210\n",
            "Epoch 3/100\n",
            "112/112 - 0s - loss: 0.3534 - accuracy: 0.8036 - val_loss: 0.3836 - val_accuracy: 0.8121\n",
            "Epoch 4/100\n",
            "112/112 - 0s - loss: 0.3465 - accuracy: 0.8075 - val_loss: 0.3763 - val_accuracy: 0.7852\n",
            "Epoch 5/100\n",
            "112/112 - 0s - loss: 0.3368 - accuracy: 0.8117 - val_loss: 0.3642 - val_accuracy: 0.8501\n",
            "Epoch 6/100\n",
            "112/112 - 0s - loss: 0.3321 - accuracy: 0.8136 - val_loss: 0.3864 - val_accuracy: 0.7718\n",
            "Epoch 7/100\n",
            "112/112 - 0s - loss: 0.3397 - accuracy: 0.8055 - val_loss: 0.3675 - val_accuracy: 0.8255\n",
            "Epoch 8/100\n",
            "112/112 - 0s - loss: 0.3283 - accuracy: 0.8198 - val_loss: 0.3604 - val_accuracy: 0.7987\n",
            "Epoch 9/100\n",
            "112/112 - 0s - loss: 0.3295 - accuracy: 0.8125 - val_loss: 0.3631 - val_accuracy: 0.8166\n",
            "Epoch 10/100\n",
            "112/112 - 0s - loss: 0.3288 - accuracy: 0.8167 - val_loss: 0.3395 - val_accuracy: 0.8322\n",
            "Epoch 11/100\n",
            "112/112 - 0s - loss: 0.3243 - accuracy: 0.8206 - val_loss: 0.3618 - val_accuracy: 0.8322\n",
            "Epoch 12/100\n",
            "112/112 - 0s - loss: 0.3211 - accuracy: 0.8223 - val_loss: 0.3484 - val_accuracy: 0.8188\n",
            "Epoch 13/100\n",
            "112/112 - 0s - loss: 0.3244 - accuracy: 0.8198 - val_loss: 0.3626 - val_accuracy: 0.8233\n",
            "Epoch 14/100\n",
            "112/112 - 0s - loss: 0.3207 - accuracy: 0.8201 - val_loss: 0.3548 - val_accuracy: 0.8233\n",
            "Epoch 15/100\n",
            "112/112 - 0s - loss: 0.3210 - accuracy: 0.8201 - val_loss: 0.3639 - val_accuracy: 0.7964\n",
            "Epoch 16/100\n",
            "112/112 - 0s - loss: 0.3189 - accuracy: 0.8251 - val_loss: 0.3771 - val_accuracy: 0.7897\n",
            "Epoch 17/100\n",
            "112/112 - 0s - loss: 0.3241 - accuracy: 0.8189 - val_loss: 0.3551 - val_accuracy: 0.8143\n",
            "Epoch 18/100\n",
            "112/112 - 0s - loss: 0.3207 - accuracy: 0.8198 - val_loss: 0.3522 - val_accuracy: 0.8277\n",
            "Epoch 19/100\n",
            "112/112 - 0s - loss: 0.3184 - accuracy: 0.8203 - val_loss: 0.3638 - val_accuracy: 0.8054\n",
            "Epoch 20/100\n",
            "112/112 - 0s - loss: 0.3159 - accuracy: 0.8189 - val_loss: 0.3528 - val_accuracy: 0.8277\n",
            "Epoch 21/100\n",
            "112/112 - 0s - loss: 0.3167 - accuracy: 0.8248 - val_loss: 0.3376 - val_accuracy: 0.8300\n",
            "Epoch 22/100\n",
            "112/112 - 0s - loss: 0.3216 - accuracy: 0.8181 - val_loss: 0.3406 - val_accuracy: 0.8479\n",
            "Epoch 23/100\n",
            "112/112 - 0s - loss: 0.3153 - accuracy: 0.8229 - val_loss: 0.3578 - val_accuracy: 0.8233\n",
            "Epoch 24/100\n",
            "112/112 - 0s - loss: 0.3166 - accuracy: 0.8223 - val_loss: 0.3555 - val_accuracy: 0.8412\n",
            "Epoch 25/100\n",
            "112/112 - 0s - loss: 0.3214 - accuracy: 0.8220 - val_loss: 0.3516 - val_accuracy: 0.8188\n",
            "Epoch 26/100\n",
            "112/112 - 0s - loss: 0.3151 - accuracy: 0.8192 - val_loss: 0.3411 - val_accuracy: 0.8479\n",
            "Epoch 27/100\n",
            "112/112 - 0s - loss: 0.3131 - accuracy: 0.8293 - val_loss: 0.3597 - val_accuracy: 0.8188\n",
            "Epoch 28/100\n",
            "112/112 - 0s - loss: 0.3148 - accuracy: 0.8153 - val_loss: 0.3546 - val_accuracy: 0.8166\n",
            "Epoch 29/100\n",
            "112/112 - 0s - loss: 0.3124 - accuracy: 0.8209 - val_loss: 0.3480 - val_accuracy: 0.8188\n",
            "Epoch 30/100\n",
            "112/112 - 0s - loss: 0.3144 - accuracy: 0.8195 - val_loss: 0.3785 - val_accuracy: 0.8054\n",
            "Epoch 31/100\n",
            "112/112 - 0s - loss: 0.3114 - accuracy: 0.8301 - val_loss: 0.3479 - val_accuracy: 0.8412\n",
            "Epoch 32/100\n",
            "112/112 - 0s - loss: 0.3182 - accuracy: 0.8122 - val_loss: 0.3433 - val_accuracy: 0.8434\n",
            "Epoch 33/100\n",
            "112/112 - 0s - loss: 0.3122 - accuracy: 0.8293 - val_loss: 0.3624 - val_accuracy: 0.8300\n",
            "Epoch 34/100\n",
            "112/112 - 0s - loss: 0.3099 - accuracy: 0.8251 - val_loss: 0.3495 - val_accuracy: 0.8233\n",
            "Epoch 35/100\n",
            "112/112 - 0s - loss: 0.3170 - accuracy: 0.8212 - val_loss: 0.3514 - val_accuracy: 0.8098\n",
            "Epoch 36/100\n",
            "112/112 - 0s - loss: 0.3138 - accuracy: 0.8234 - val_loss: 0.3574 - val_accuracy: 0.8121\n",
            "Epoch 37/100\n",
            "112/112 - 0s - loss: 0.3145 - accuracy: 0.8206 - val_loss: 0.3437 - val_accuracy: 0.8210\n",
            "Epoch 38/100\n",
            "112/112 - 0s - loss: 0.3085 - accuracy: 0.8243 - val_loss: 0.3461 - val_accuracy: 0.8501\n",
            "Epoch 39/100\n",
            "112/112 - 0s - loss: 0.3093 - accuracy: 0.8237 - val_loss: 0.3325 - val_accuracy: 0.8300\n",
            "Epoch 40/100\n",
            "112/112 - 0s - loss: 0.3125 - accuracy: 0.8201 - val_loss: 0.3515 - val_accuracy: 0.8389\n",
            "Epoch 41/100\n",
            "112/112 - 0s - loss: 0.3096 - accuracy: 0.8298 - val_loss: 0.3663 - val_accuracy: 0.8076\n",
            "Epoch 42/100\n",
            "112/112 - 0s - loss: 0.3095 - accuracy: 0.8270 - val_loss: 0.3324 - val_accuracy: 0.8412\n",
            "Epoch 43/100\n",
            "112/112 - 0s - loss: 0.3084 - accuracy: 0.8262 - val_loss: 0.3477 - val_accuracy: 0.8367\n",
            "Epoch 44/100\n",
            "112/112 - 0s - loss: 0.3115 - accuracy: 0.8240 - val_loss: 0.3495 - val_accuracy: 0.8166\n",
            "Epoch 45/100\n",
            "112/112 - 0s - loss: 0.3073 - accuracy: 0.8276 - val_loss: 0.3607 - val_accuracy: 0.8076\n",
            "Epoch 46/100\n",
            "112/112 - 0s - loss: 0.3091 - accuracy: 0.8321 - val_loss: 0.3628 - val_accuracy: 0.8591\n",
            "Epoch 47/100\n",
            "112/112 - 0s - loss: 0.3083 - accuracy: 0.8215 - val_loss: 0.3498 - val_accuracy: 0.8479\n",
            "Epoch 48/100\n",
            "112/112 - 0s - loss: 0.3059 - accuracy: 0.8248 - val_loss: 0.3549 - val_accuracy: 0.8412\n",
            "Epoch 49/100\n",
            "112/112 - 0s - loss: 0.3081 - accuracy: 0.8223 - val_loss: 0.3417 - val_accuracy: 0.8300\n",
            "Epoch 50/100\n",
            "112/112 - 0s - loss: 0.3041 - accuracy: 0.8256 - val_loss: 0.3442 - val_accuracy: 0.8412\n",
            "Epoch 51/100\n",
            "112/112 - 0s - loss: 0.3076 - accuracy: 0.8215 - val_loss: 0.3557 - val_accuracy: 0.8434\n",
            "Epoch 52/100\n",
            "112/112 - 0s - loss: 0.3063 - accuracy: 0.8318 - val_loss: 0.3375 - val_accuracy: 0.8345\n",
            "Epoch 53/100\n",
            "112/112 - 0s - loss: 0.3051 - accuracy: 0.8284 - val_loss: 0.3553 - val_accuracy: 0.8076\n",
            "Epoch 54/100\n",
            "112/112 - 0s - loss: 0.3095 - accuracy: 0.8279 - val_loss: 0.3396 - val_accuracy: 0.8255\n",
            "Epoch 55/100\n",
            "112/112 - 0s - loss: 0.3071 - accuracy: 0.8312 - val_loss: 0.3632 - val_accuracy: 0.8613\n",
            "Epoch 56/100\n",
            "112/112 - 0s - loss: 0.3087 - accuracy: 0.8256 - val_loss: 0.3540 - val_accuracy: 0.8434\n",
            "Epoch 57/100\n",
            "112/112 - 0s - loss: 0.3086 - accuracy: 0.8229 - val_loss: 0.3477 - val_accuracy: 0.8456\n",
            "Epoch 58/100\n",
            "112/112 - 0s - loss: 0.3072 - accuracy: 0.8262 - val_loss: 0.3395 - val_accuracy: 0.8456\n",
            "Epoch 59/100\n",
            "112/112 - 0s - loss: 0.3055 - accuracy: 0.8231 - val_loss: 0.3460 - val_accuracy: 0.8210\n",
            "Epoch 60/100\n",
            "112/112 - 0s - loss: 0.3074 - accuracy: 0.8248 - val_loss: 0.3490 - val_accuracy: 0.8255\n",
            "Epoch 61/100\n",
            "112/112 - 0s - loss: 0.3053 - accuracy: 0.8223 - val_loss: 0.3414 - val_accuracy: 0.8591\n",
            "Epoch 62/100\n",
            "112/112 - 0s - loss: 0.3058 - accuracy: 0.8265 - val_loss: 0.3331 - val_accuracy: 0.8412\n",
            "Epoch 63/100\n",
            "112/112 - 0s - loss: 0.3052 - accuracy: 0.8251 - val_loss: 0.3589 - val_accuracy: 0.8412\n",
            "Epoch 64/100\n",
            "112/112 - 0s - loss: 0.3032 - accuracy: 0.8290 - val_loss: 0.3640 - val_accuracy: 0.8210\n",
            "Epoch 65/100\n",
            "112/112 - 0s - loss: 0.3080 - accuracy: 0.8296 - val_loss: 0.3497 - val_accuracy: 0.8389\n",
            "Epoch 66/100\n",
            "112/112 - 0s - loss: 0.3036 - accuracy: 0.8290 - val_loss: 0.3734 - val_accuracy: 0.8210\n",
            "Epoch 67/100\n",
            "112/112 - 0s - loss: 0.3032 - accuracy: 0.8262 - val_loss: 0.3492 - val_accuracy: 0.8367\n",
            "Epoch 68/100\n",
            "112/112 - 0s - loss: 0.3038 - accuracy: 0.8315 - val_loss: 0.3684 - val_accuracy: 0.8076\n",
            "Epoch 69/100\n",
            "112/112 - 0s - loss: 0.3028 - accuracy: 0.8270 - val_loss: 0.3486 - val_accuracy: 0.8233\n",
            "Epoch 70/100\n",
            "112/112 - 0s - loss: 0.3020 - accuracy: 0.8279 - val_loss: 0.3368 - val_accuracy: 0.8166\n",
            "Epoch 71/100\n",
            "112/112 - 0s - loss: 0.3059 - accuracy: 0.8251 - val_loss: 0.3447 - val_accuracy: 0.8568\n",
            "Epoch 72/100\n",
            "112/112 - 0s - loss: 0.3016 - accuracy: 0.8290 - val_loss: 0.3730 - val_accuracy: 0.8210\n",
            "Epoch 73/100\n",
            "112/112 - 0s - loss: 0.3057 - accuracy: 0.8296 - val_loss: 0.3421 - val_accuracy: 0.8434\n",
            "Epoch 74/100\n",
            "112/112 - 0s - loss: 0.3035 - accuracy: 0.8307 - val_loss: 0.3392 - val_accuracy: 0.8412\n",
            "Epoch 75/100\n",
            "112/112 - 0s - loss: 0.3031 - accuracy: 0.8284 - val_loss: 0.3554 - val_accuracy: 0.8434\n",
            "Epoch 76/100\n",
            "112/112 - 0s - loss: 0.3019 - accuracy: 0.8248 - val_loss: 0.3602 - val_accuracy: 0.8255\n",
            "Epoch 77/100\n",
            "112/112 - 0s - loss: 0.3056 - accuracy: 0.8215 - val_loss: 0.3618 - val_accuracy: 0.8412\n",
            "Epoch 78/100\n",
            "112/112 - 0s - loss: 0.3022 - accuracy: 0.8273 - val_loss: 0.3601 - val_accuracy: 0.8345\n",
            "Epoch 79/100\n",
            "112/112 - 0s - loss: 0.3023 - accuracy: 0.8276 - val_loss: 0.3607 - val_accuracy: 0.8434\n",
            "Epoch 80/100\n",
            "112/112 - 0s - loss: 0.3016 - accuracy: 0.8290 - val_loss: 0.3451 - val_accuracy: 0.8412\n",
            "Epoch 81/100\n",
            "112/112 - 0s - loss: 0.3001 - accuracy: 0.8343 - val_loss: 0.3633 - val_accuracy: 0.8434\n",
            "Epoch 82/100\n",
            "112/112 - 0s - loss: 0.3027 - accuracy: 0.8312 - val_loss: 0.3672 - val_accuracy: 0.7964\n",
            "Epoch 83/100\n",
            "112/112 - 0s - loss: 0.3003 - accuracy: 0.8273 - val_loss: 0.3597 - val_accuracy: 0.8322\n",
            "Epoch 84/100\n",
            "112/112 - 0s - loss: 0.3016 - accuracy: 0.8279 - val_loss: 0.3585 - val_accuracy: 0.8233\n",
            "Epoch 85/100\n",
            "112/112 - 0s - loss: 0.2994 - accuracy: 0.8279 - val_loss: 0.3602 - val_accuracy: 0.8255\n",
            "Epoch 86/100\n",
            "112/112 - 0s - loss: 0.3020 - accuracy: 0.8265 - val_loss: 0.3651 - val_accuracy: 0.8121\n",
            "Epoch 87/100\n",
            "112/112 - 0s - loss: 0.3010 - accuracy: 0.8324 - val_loss: 0.3531 - val_accuracy: 0.8300\n",
            "Epoch 88/100\n",
            "112/112 - 0s - loss: 0.3063 - accuracy: 0.8254 - val_loss: 0.3488 - val_accuracy: 0.8233\n",
            "Epoch 89/100\n",
            "112/112 - 0s - loss: 0.3014 - accuracy: 0.8307 - val_loss: 0.3622 - val_accuracy: 0.8277\n",
            "Epoch 90/100\n",
            "112/112 - 0s - loss: 0.3006 - accuracy: 0.8282 - val_loss: 0.3420 - val_accuracy: 0.8277\n",
            "Epoch 91/100\n",
            "112/112 - 0s - loss: 0.2992 - accuracy: 0.8251 - val_loss: 0.3495 - val_accuracy: 0.8591\n",
            "Epoch 92/100\n",
            "112/112 - 0s - loss: 0.3023 - accuracy: 0.8279 - val_loss: 0.3458 - val_accuracy: 0.8501\n",
            "Epoch 93/100\n",
            "112/112 - 0s - loss: 0.2962 - accuracy: 0.8340 - val_loss: 0.3468 - val_accuracy: 0.8434\n",
            "Epoch 94/100\n",
            "112/112 - 0s - loss: 0.3000 - accuracy: 0.8268 - val_loss: 0.3492 - val_accuracy: 0.8523\n",
            "Epoch 95/100\n",
            "112/112 - 0s - loss: 0.3022 - accuracy: 0.8262 - val_loss: 0.3722 - val_accuracy: 0.8188\n",
            "Epoch 96/100\n",
            "112/112 - 0s - loss: 0.3000 - accuracy: 0.8231 - val_loss: 0.3632 - val_accuracy: 0.8121\n",
            "Epoch 97/100\n",
            "112/112 - 0s - loss: 0.3002 - accuracy: 0.8329 - val_loss: 0.3452 - val_accuracy: 0.8322\n",
            "Epoch 98/100\n",
            "112/112 - 0s - loss: 0.3021 - accuracy: 0.8273 - val_loss: 0.3485 - val_accuracy: 0.8434\n",
            "Epoch 99/100\n",
            "112/112 - 0s - loss: 0.2981 - accuracy: 0.8304 - val_loss: 0.3705 - val_accuracy: 0.8300\n",
            "Epoch 100/100\n",
            "112/112 - 0s - loss: 0.2979 - accuracy: 0.8284 - val_loss: 0.3615 - val_accuracy: 0.8367\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8f0237d208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmdbs-Ix6GrA"
      },
      "source": [
        "We can also work with the model on the test set, although we won't get as strong of a picture of what our results look like. We can do this through model.evaluate, which takes in the test set and the batch_size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbMljNnabMEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2ad5a7-69f9-4dd2-f4a1-e892e1bfd4cb"
      },
      "source": [
        "model.evaluate(test_inputs, test_targets, batch_size = batch_size, verbose = 2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14/14 - 0s - loss: 0.3668 - accuracy: 0.8013\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.36681556701660156, 0.8013392686843872]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWCjl_eYZZxA"
      },
      "source": [
        "Our results are not too bad! We're getting around 83% on our validation accuracy, and we haven't even used any special techniques yet. The first thing that we can do is hyperparameter tuning - changing the size of the hidden layers, possibly the batch size, the number of epochs, and so on. \n",
        "\n",
        "We definitely also need to be mindful of overfitting - there are a few techniques that we can use to accomplish this which are mostly beyond the scope of this bootcamp - but be sure to keep an eye out for future sessions where we may cover topics like these! However, a very easy that method we can employ is dropout, where we effectively ignore a certain percentage of inputs for each iteration, and this allows us to learn various parts of the data better. \n",
        "\n",
        "For now, instead of constantly running this for 100 epochs even when validation loss continues to increase, we will use Tensorflow's `EarlyStopping` mechanism to stop fitting the model after a \"certain number of iterations\" have passed where the loss was higher than the previous epoch's loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5ivxE3Tbsvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a735a2-0319-4ea2-fc9f-59e6c0f83f49"
      },
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(patience = 5) # This patience is the \"certain number of iterations\" mentioned above\n",
        "model.fit(train_inputs, train_targets, batch_size = batch_size, \n",
        "          epochs = epochs, validation_data=(validation_inputs, validation_targets), \n",
        "          verbose = 2, \n",
        "          callbacks = [stop_early]\n",
        "          )  "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "112/112 - 0s - loss: 0.2983 - accuracy: 0.8265 - val_loss: 0.3534 - val_accuracy: 0.8389\n",
            "Epoch 2/100\n",
            "112/112 - 0s - loss: 0.2986 - accuracy: 0.8256 - val_loss: 0.3732 - val_accuracy: 0.8277\n",
            "Epoch 3/100\n",
            "112/112 - 0s - loss: 0.2962 - accuracy: 0.8329 - val_loss: 0.3459 - val_accuracy: 0.8456\n",
            "Epoch 4/100\n",
            "112/112 - 0s - loss: 0.2990 - accuracy: 0.8315 - val_loss: 0.3435 - val_accuracy: 0.8277\n",
            "Epoch 5/100\n",
            "112/112 - 0s - loss: 0.2976 - accuracy: 0.8351 - val_loss: 0.3525 - val_accuracy: 0.8456\n",
            "Epoch 6/100\n",
            "112/112 - 0s - loss: 0.2989 - accuracy: 0.8293 - val_loss: 0.3497 - val_accuracy: 0.8456\n",
            "Epoch 7/100\n",
            "112/112 - 0s - loss: 0.2986 - accuracy: 0.8298 - val_loss: 0.3540 - val_accuracy: 0.8255\n",
            "Epoch 8/100\n",
            "112/112 - 0s - loss: 0.2977 - accuracy: 0.8284 - val_loss: 0.3483 - val_accuracy: 0.8389\n",
            "Epoch 9/100\n",
            "112/112 - 0s - loss: 0.2981 - accuracy: 0.8332 - val_loss: 0.3612 - val_accuracy: 0.8568\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8efec32fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grxOQSEPcI0Z"
      },
      "source": [
        "Note, however, that the main drawback to using the `EarlyStopping` mechanism is that we do not get to see the broader set of validation accuracy results - this leaves us to believe that our maximum accuracy is lower than it actually is, and does not allow us to get a sense of general consistency of performance - every percent in accuracy counts!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVYUvvY2_oyF"
      },
      "source": [
        "#**Appendix**\n",
        "Here are the preprocessing steps for the audiobooks customer dataset that we didn't touch upon for the neural networks component of the programming session. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTLD5byaTjfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e0c40b8-f88a-4a85-a7f5-86aeb16b39ed"
      },
      "source": [
        "# preprocessing steps\n",
        "csv_data = np.loadtxt('/content/AI-Penn/Session 4 - Neural Networks (News Outreach)/Audiobooks_data_3.csv', delimiter = ',')\n",
        "unscaled_inputs = csv_data [:,1:-1]\n",
        "targets = csv_data[:,-1]\n",
        "\n",
        "# balancing dataset - ensuring that when we train, we are able to ensure that we have similar number of each so\n",
        "# that our model does not learn one more than the other\n",
        "num_one_targets = int(np.sum(targets))  ## find number of 1 boolean values for balancing\n",
        "num_zero_targets = 0\n",
        "removed_index = []\n",
        "for i in range (targets.shape[0]):\n",
        "    if(targets[i] == 0):\n",
        "        num_zero_targets += 1\n",
        "        if (num_zero_targets > num_one_targets):\n",
        "            removed_index.append(i)\n",
        "unscaled_inputs_equalpriors = np.delete (unscaled_inputs, removed_index, axis = 0)\n",
        "target_equal_priors = np.delete (targets, removed_index, axis = 0)\n",
        "\n",
        "# Scaling the set using sklearn's preprocessing \n",
        "scaled_inputs = preprocessing.scale(unscaled_inputs_equalpriors)\n",
        "\n",
        "#shuffling - so we effectively have a randomized process of selecting values from the dataset for when it is partitioned\n",
        "indices_shuffled = np.arange(scaled_inputs.shape[0])\n",
        "np.random.shuffle(indices_shuffled)\n",
        "shuffled_inputs = scaled_inputs[indices_shuffled]\n",
        "shuffled_targets = target_equal_priors[indices_shuffled]\n",
        "\n",
        "# splitting data into train, validation, and test -- currently 80:10:10 but can be tweaked\n",
        "samples_count = shuffled_inputs.shape[0]\n",
        "samples_count = scaled_inputs.shape[0]\n",
        "train_samples_count = int(0.8*samples_count) \n",
        "validation_samples_count = int(0.1*samples_count)\n",
        "test_samples_count = samples_count - train_samples_count - validation_samples_count \n",
        "\n",
        "#Setting the train, validation, and test inputs and outputs to incorporate particular ranges of the set \n",
        "train_inputs = shuffled_inputs[:train_samples_count]\n",
        "train_targets = shuffled_targets[:train_samples_count]\n",
        "#train_inputs = scaled_inputs[:train_samples_count]\n",
        "#train_targets = target_equal_priors[:train_samples_count]\n",
        "\n",
        "validation_inputs = shuffled_inputs[train_samples_count:(train_samples_count + validation_samples_count)]\n",
        "validation_targets = shuffled_targets[train_samples_count:(train_samples_count + validation_samples_count)]\n",
        "\n",
        "test_inputs = shuffled_inputs[(train_samples_count + validation_samples_count):]\n",
        "test_targets = shuffled_targets[(train_samples_count + validation_samples_count):]\n",
        "\n",
        "\n",
        "print(np.sum(train_targets), train_samples_count, np.sum(train_targets)/train_samples_count)\n",
        "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets)/validation_samples_count)\n",
        "print(np.sum(test_targets), test_samples_count, np.sum(test_targets)/test_samples_count)\n",
        "\n",
        "# Saving the numpy arrays as .npz files - this is what is in the Github that we imported to construct our neural network from\n",
        "np.savez('Audiobooks_train_data_2', inputs=train_inputs, targets=train_targets)\n",
        "np.savez('Audiobooks_validation_data_2',inputs=validation_inputs, targets=validation_targets)\n",
        "np.savez('Audiobooks_test_data_2', inputs=test_inputs, targets=test_targets)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1803.0 3579 0.5037720033528919\n",
            "225.0 447 0.5033557046979866\n",
            "209.0 448 0.46651785714285715\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}